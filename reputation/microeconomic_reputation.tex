\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{hyperref}

\newtheorem{conclusion}{Conclusion}[section]

\title{Strategies and reputation - a microeconomic description of the Golem marketplace}
\author{Jan Betley}

\begin{document}
\maketitle
\section{Introduction}
\subsection{Reader notes}

Every important statement is labelled as a \underline{Conclusion} - consider skimming through them if you're not interested in the reasoning.

This document is quite formal, but please don't expect clean, bulletproof implications only. The purpose of this document is to
present the topic and conclusions while maintaining resonable level of brievity.

\subsection{Abstract}
This document provides a simple microeconomic model of the Golem marketplace. 
Following questions are answered within the model:

\begin{itemize}
\item What do we exactly mean by the "reputation"?
\item What is the exact problem we hope to solve with the reputation mechanism?
\item What are the other, non-reputation approaches to the main problem?
\item How to measure the quality of our solutions? I.e. how to know if we succeeded?
\end{itemize}


The problem is approached from the highest point of view - no specific solutions are proposed, only general classes of solutions are discussed.
With an exception of a few details/examples this document describes just a "general" marketplace (replace "provider/requestor" with "seller/buyer").

\subsection{Definitions}

\textbf{Agent} - A decision-making entity (person, company, etc.).
\newline
\textbf{Utility} - A total happiness of an agent.
\newline
\textbf{Utility function $U$} - A function $U: StateOfTheWorld -> \mathbb{R}$, defined for a particular agent, that represents the agents utility in a given situation.
\newline
\textbf{Expected utility} - For every possible decision an agent can make ($D$) and every possible state of the world $w$, there is some probability $P: (D, w) \rightarrow [0, 1]$  that given decision will lead to the given state of the world.
Agents don't have the full knowledge about $P$, they know only some information $I$ and an estimation of $P$ based on this information: $P_I: (D, w) \rightarrow [0, 1]$. The expected utility $E: (I, U, D) -> \mathbb{R}$ is defined as follows:

\begin{equation}
E(I, U, D) = \sum_{w \in all\_possible\_world\_states}U(w) * P_I(D, w)
\end{equation}

We'll usually use a shorter notation $E(I, U, D) = E_A(D)$ to describe "expected utility of agent A, who has information $I$ and utility function $D$".

In other words, for agent $A$ and decision $D$, $E_A(D)$ is "how happy agent $A$ expects to be if they do $D$".
\newline

\textbf{Golem} - [TODO better definition?] Everything that influences the utility resulting from the interactions with the Golem Network protocol - the protocol itself, available software, state of the market etc.
\newline

\subsection{Additional assumptions}

\begin{enumerate}
\item The Expected Utility Hypothesis\footnote{\href{https://en.wikipedia.org/wiki/Expected\_utility\_hypothesis}{https://en.wikipedia.org/wiki/Expected\_utility\_hypothesis}}: every agent tries to maximize their expected utility.

\item Every agent can costlessy access and analyze all of the publicly available information about Golem. Thanks to this assumption we can remove $I$ from the equations - we no longer
    care about "what agent knows", but only about "what really is". This is a major simplification that might not be a good approximation of the "real world" Golem market, but we
    accept it for the sake of the brevity of this document. Also "truth about Golem is known" is an ideal world we would like to live in.

\item The expected utility and money are interchangeable, that is - for every agent $A$ and a pair of decisions $(D_1, D_2): E_A(D_1) < E_A(D_2)$, there is an amount of money X that
    can be given to the agent so that $E_A(D_1) + U_A(X) = E_A(D_2)$. This way we can treat utilities as if they were money, and thus compare them between different agents.

\item Lets define the Golem Value as:

\begin{equation}
    V_G = \sum_{A \in all\_agents\_using\_Golem}E_A(USE\_GOLEM)
\end{equation}
(Note that - because of assumptions 1 and 2 - $E_A(USE\_GOLEM)$ is always positive, decision "use Golem" is only made by agents who profit from it).

We assume that maximization of $V_G$ is one of the goals of the Golem Factory - and from the point of view of this document, the only goal.
\end{enumerate}

\section{The purpose of the reputation}
\subsection{Notation}

We'll be using a bunch of different symbols, but they follow a clear common pattern, so please don't be frightened.

\begin{itemize}
\item{$V_A(a)$ is the total value of the agreement $a$ from the POV of agent $A$. As per assumption 3, this is "utility expressed as money".\footnote{
I.e. $V_A(a) = X$ means "when agent $A$ takes part in the agreement $a$, their hapiness changes as if they were given $X$ money".}}
\item{$V_P(a)$/$V_R(a)$ are agreement values from the POV of (respectively) provider/requestor.}
\item{$V_{PN}(a)$/$V_{RN}$ are nominal (i.e. negotiated) values of the agreement. They equal $V_P(a)$/$V_R(a)$ if neither side breached the agreement $a$.}
\item{$V_{AL}(a)$ is the value lost by agent $A$ because of the other side breaching the agreement $a$.}
\item{$V_{AG}(a)$ is the value gained by agent $A$ when they break the agreement $a$.}
\item{$V_{PL}$, $V_{RL}$, $V_{PG}$, $V_{RG}$ are $V_{AL}$/$V_{AG}$ from the POV of provider/requestor.}
\item{$E_{[whatever]}(a)$ is the estimated value of $V_{[whatever]}(a)$, when we're not certain about the outcome. 
E.g. agent $A$ signs an agreement $a$ because they calculated high enough $E_A(a)$. 
Only after the agreement ends the agent knows the final $V_A(a)$ (and thus knows if signing it was a good decision).}
\item{$C_A$ is the cost of the participation in the Golem market for agent $A$ that is not related to any particular agreement 
(e.g. opportunity cost\footnote{\href{https://en.wikipedia.org/wiki/Opportunity\_cost}{https://en.wikipedia.org/wiki/Opportunity\_cost}} of the hardware offered on the market, 
or the cost of writing the requestor agent).}
\end{itemize}

\subsection{Dissolving the Golem Value}

Utilizing the above notation, we can rewrite the $V_G$ equation as:

\begin{equation}
    V_G = \sum_{a \in all\_agreements}(V_P(a) + V_R(a)) - \sum_{A \in all\_agents\_using\_Golem}C_A
\end{equation}

The following equation and it's counterpart from the requestor POV are always true:

\begin{equation}
    V_P(a) = V_{PN}(a) + V_{PG}(a) - V_{PL}(a)
\end{equation}

By placing them in the previous equation, we get:

\begin{equation}
\begin{split}
    V_G = \sum_{a \in all\_agreements}(V_{PN}(a) + V_{PG}(a) - V_{PL}(a) + V_{RN}(a) + V_{RG}(a) - V_{RL}(a)) \\
            - \sum_{A \in all\_agents\_using\_Golem}C_A \\
\end{split}
\end{equation}
\begin{equation}
\begin{split}
    V_G = \sum_{a \in all\_agreements}(V_{PN}(a) + V_{PG}(a)) \\
          - \sum_{a \in all\_agreements}(V_{PL}(a)- V_{RG}(a)) \\
          - \sum_{a \in all\_agreements}(V_{RL}(a)- V_{PG}(a)) \\
          - \sum_{A \in all\_agents\_using\_Golem}C_A
\end{split}
\end{equation}

Important note here is that both $\sum_{a \in all\_agreements}(V_{PL}(a)- V_{RG}(a))$ and $\sum_{a \in all\_agreements}(V_{RL}(a)- V_{PG}(a))$ are positive: 
when someone breaks the agreement, the harm done to the victim is usually greater then the offenders gain\footnote{TODO: examples/proof/justification?}.
Let's thus define one more symbol:
\begin{itemize}
\item{$V_L(a) = (V_{PL}(a)- V_{RG}(a)) + (V_{RL}(a)- V_{PG}(a))$ - the total value lost because of agents breaking the terms of agreement $a$.}
\end{itemize}
and use it to rewrite the main equation one last time:

\begin{equation}
\begin{split}
    V_G = \sum_{a \in all\_agreements}(V_{PN}(a) + V_{RN}(a) - V_L(a)) \\
          - \sum_{A \in all\_agents\_using\_Golem}C_A
\end{split}
\end{equation}

This equation defines few general ways to increase the $V_G$:

\begin{enumerate}
\item Increase the number of agreements
\item Increase the average nominal value of the agreement (e.g. by providing additional capabilities, like internet connectivity or GPU access)
\item Decrease the average value lost because of breaches of the agreements ($V_L(a)$)
\item Decrease the cost of the participation in the Golem market (e.g. by creating a better SDKs)
\end{enumerate}

Note that these ways interact with each other: e.g. if we improve 2 by implementing some features that will be hard to use, we'll also worsen 4.
Or: the better is the average nominal value of the agreement, the more agreements we'll expect to have.
Or - what is important from the POV of this document - if we implement complex safeguards against cheating, they might have negative impact on all the other points.

Keeping that in mind, the rest of the document is aimed at the third direction from the above list.

\subsection{Honest strategy}

Let's assume $A$ is an agent who never breaks an agreement, \footnote{This assumption only simplifies the description, but we don't really need it}
i.e.  $V_AG(a) = 0$, so $V_A(a) = V_{AN}(a) - V_{AL}(a)$. [TODO]

% Let's define the fulfillment level of the agreement $a$ from the point of view of agent $A$ as:
% 
% \begin{equation}
%     F_A(a) = \frac{V_A(a)}{V_{AN}(a)}
% \end{equation}
% 
% That is: if we received everything we agreed on, $F_A(a) = 1$. If all our agreement-related costs are covered, but we got nothing more, $F_A(a) = 0$.
% If we lost on the agreement, $F_A(a) < 0$.\footnote{Values over 1 are also possible, but not really important, as they would be rare exceptions only.}.
% 
% The higher is the $F_A(a)$, the more "honest" was the behaviour of the agent who was trading with $A$.
% The algorithm behind agent decisions is called a strategy. The more honestly agent behaves, the more honest is the strategy.\footnote{
% This is not a precise definition, but we don't need a precise definition here.}
% Agent's intentions don't matter here - there's no difference if an agent breaks an agreement on purpose or accidentally.
% 
% Agent always tries to maximize their own utility (the "expected utility maximization" assumption), in other words: agent always selects the most profitable strategy.
% Let's now paraphrase the third goal from the previous section as:

\begin{conclusion}

The final purpose of the reputation system on Golem is to make honest strategies more profitable than dishonest strategies.

\label{main purpose conclusion}
\end{conclusion}


\subsection{Defining reputation}

There are few different ways to reduce the profitability of dishonest strategies:

\begin{itemize}
\item{Make dishonest strategies not available at all, e.g. ensure that debit note acceptance forces payment.}
\item{Make dishonest strategies hard to implement, e.g. hide/obfuscate some important components of Golem.}
\item{Add some mechanics that directly penalize dishonesty, e.g. require deposits and confiscate them when dishonesty is proven.}
\end{itemize}

Neither of them has anything to do with the "reputation". So, what exactly is the reputation?

Imagine an honest agent who considers signing an agreement $a$ with agent $A$. The decision algorithm can be rougly summarized as
\begin{enumerate}
\item{Calculate the expected value of the decision \textbf{not} to sign the agreement, $E(\neg a)$}
\item{Calculate the expected value of the decision to sign the agreement $E(a)$. 
    According to eq. 8 it depends on the nominal value of the agreement 
    and the estimated fulfillment level of the agreement: $E(a) = V_N(a) * F_A(a)$}
\item{Sign the agreement if $E_N(a) * H(S_A) > E(\neg a)$}
\end{enumerate}

This leads us to a simple observation: the less honest agent is, the better $E_N$ they have to offer to find someone who'll trade with them, and thus to:

\begin{conclusion}

"Reputation system" is an attempt to solve the problem defined in the conclusion \ref{main purpose conclusion} in the following way:

\begin{itemize}
    \item{Make some additional information available to the market participants}
    \item{This information can be used to estimate the "honesty index" of an agent, and thus improves the accuracy of the total agreement value estimation}
    \item{The more accurate is the total agreement value estimation, the less profitable it is to trade with dishonest agents}
    \item{The less profitable it is to trade with dishonest agents, the fewer/worse agreements they have}
    \item{The fewer/worse agreements dishonest agents have, the less profitable are dishonest strategies}
\end{itemize}

\end{conclusion}


\section{Final notes}

1. "How good is reputation" == "estimate honesty factor"
2. "How useful is reputation" is "utilization of the honesty factor estimation"
--> Two separate topics: provide the information and ensure the information is used.

E.g. what if there is only a single requestor? Or there are less providers the necessary?
--> We must take care about the market.

"Fulfillment levels" could be different for providers /requestors (and compensated by $V_N$).
Possible balance: super-honest providers and equally dishonest requestors.

\end{document}
